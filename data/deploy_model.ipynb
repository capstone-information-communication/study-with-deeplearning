{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deploy_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"rNuhpNSl3sip","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654682769202,"user_tz":-540,"elapsed":2102,"user":{"displayName":"와바라바덥덥","userId":"12824072155339384390"}},"outputId":"9b138641-b8b8-4c6f-bc67-5b0b8eacefdb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/My Drive/ner_example/"],"metadata":{"id":"4kJE3GUz3yhq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654682769203,"user_tz":-540,"elapsed":5,"user":{"displayName":"와바라바덥덥","userId":"12824072155339384390"}},"outputId":"d5d9c4c8-7b1e-480c-e1b4-88b97018c7c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/ner_example\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1jONbs_YweK"},"outputs":[],"source":["!pip install flask_ngrok\n","!pip install transformers\n","!pip install konlpy"]},{"cell_type":"code","source":["import json\n","import numpy as np\n","\n","from main import order, multiple, blank\n","from transformers import TFBertModel, BertTokenizer\n","from tensorflow.keras.models import load_model\n","from flask import Flask, request, Response, jsonify\n","from flask_ngrok import run_with_ngrok\n","from konlpy.tag import Okt"],"metadata":{"id":"FPWqwdkAZUs0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["okt = Okt()\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","model = load_model('best_model.h5', custom_objects={'TFBertModel': TFBertModel})"],"metadata":{"id":"ThWDVdNV4CDC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654682834631,"user_tz":-540,"elapsed":35059,"user":{"displayName":"와바라바덥덥","userId":"12824072155339384390"}},"outputId":"42eb7cf0-c84d-4401-b461-320ae677bb35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"]}]},{"cell_type":"code","source":["# 모델 예측에 필요한 정적 변수\n","max_len = 88\n","index_to_ner = {0: 'PER_B', 1: 'DAT_B', 2: '-', 3: 'ORG_B', 4: 'CVL_B', 5: 'NUM_B', 6: 'LOC_B', 7: 'EVT_B', 8: 'TRM_B', 9: 'TRM_I', 10: 'EVT_I', 11: 'PER_I', 12: 'CVL_I', 13: 'NUM_I', 14: 'TIM_B', 15: 'TIM_I', 16: 'ORG_I', 17: 'DAT_I', 18: 'ANM_B', 19: 'MAT_B', 20: 'MAT_I', 21: 'AFW_B', 22: 'FLD_B', 23: 'LOC_I', 24: 'AFW_I', 25: 'PLT_B', 26: 'FLD_I', 27: 'ANM_I', 28: 'PLT_I', 29: '[PAD]'}"],"metadata":{"id":"BrW4Zudy4Q9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_total_dict(result_list):\n","  total_dict = {}\n","  for result in result_list:\n","    for word, tag in result:\n","      if tag in total_dict:\n","        total_dict[tag].append(word)\n","      else:\n","        total_dict[tag] = [word]\n","  return total_dict\n","\n","def ner_inference(test_sentence):\n","  morphs = okt.morphs(test_sentence)\n","  test_sentence = \" \".join(morphs)\n","  tokenized_sentence = np.array([tokenizer.encode(test_sentence, max_length=max_len, truncation=True, padding='max_length')])\n","  tokenized_mask = np.array([[int(x!=1) for x in tokenized_sentence[0].tolist()]])\n","  ans = model.predict([tokenized_sentence, tokenized_mask])\n","  ans = np.argmax(ans, axis=2)\n","\n","  tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence[0])\n","  new_tokens, new_labels = [], []\n","  for token, label_idx in zip(tokens, ans[0]):\n","    if token=='[CLS]' or token == '[SEP]' or token == '[PAD]':\n","      pass\n","    else:\n","      new_tokens.append(token)\n","      new_labels.append(index_to_ner[label_idx])\n","  return [new_tokens, new_labels]\n","\n","def get_result(new_tokens, new_labels):\n","  result = []\n","  word, label = \"\", \"\"\n","  for item in zip(new_tokens, new_labels):\n","    if not item[0].startswith('##'):\n","      result.append((word, label))\n","      word = item[0]\n","    else:\n","      word += item[0][2:]\n","    label = item[1]\n","  result.append((word, label))\n","  result.pop(0)\n","  return result"],"metadata":{"id":"j0yYVQxS4lWR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","app = Flask(__name__)\n","\n","@app.route('/')\n","def index():\n","  return \"hello flask\"\n","\n","@app.route('/workbook-with-text', methods=['POST'])\n","def make_question_response():\n","  data = request.json\n","  text = data['text']\n","  sentence_list, input_list = [], []\n","\n","  predict_list = []\n","  sentence_list = text.split('.')\n","  for sentence in sentence_list:\n","    token, label = ner_inference(sentence)\n","    predict_list.append(get_result(token, label))\n","\n","  question_list = []\n","  total_dict = get_total_dict(predict_list)\n","  for predict in predict_list:\n","    if predict:\n","      question_list.append(blank('PER_B', predict))\n","      question_list.append(multiple('PER_B', predict, total_dict))\n","  question_list.append(order(predict_list))\n","  random.shuffle(question_list)\n","  return jsonify(question_list), 200"],"metadata":{"id":"JJhfEzTuZcJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_with_ngrok(app)\n","app.run()"],"metadata":{"id":"8kPlwkEDaRQp"},"execution_count":null,"outputs":[]}]}